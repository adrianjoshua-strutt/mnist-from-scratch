# MNIST From Scratch
#### Building a simple feedforward neural network (FNN) from scratch without relying on PyTorch or Tensorflow/Keras, using only numpy to solve the MNIST dataset.

# Background:

I build a simple feedforward neural network (FNN) from scratch without relying on PyTorch or Tensorflow/Keras, using only numpy to solve the MNIST dataset.
It employs vanilla stochastic gradient descent and the MSELoss.
The project does not use Pytorch or Tensorflow/Keras and is only coded using numpy.

# Key Takeaways:
### 1. A Deeper Understanding of Neural Networks
Implementing backpropagation, MSE loss, fully connected layers, and stochastic gradient descent from scratch.
### 2. Getting Used to Vanilla Numpy
Gain familiarity with pure numpy.

# Results:

### The Training Accuracy and Loss

![The Training Accuracy and Loss](./Docs/Training.png?raw=true "The Training Accuracy and Loss")
