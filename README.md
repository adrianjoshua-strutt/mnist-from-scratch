# MNIST From Scratch
#### A simple feedforward neural network (FNN) from scratch using numpy to solve MNIST

# Background:

I build a simple feedforward neural network (FNN) from scratch without relying on PyTorch or Tensorflow/Keras, using only numpy to solve the MNIST dataset.
It employs vanilla stochastic gradient descent and the MSELoss.
The project does not use Pytorch or Tensorflow/Keras and is only coded using numpy.


# Key Takeaways:
### 1. A Deeper Understanding of Neural Networks
Implementing backpropagation, MSE loss, fully connected layers, and stochastic gradient descent from scratch.
### 2. Getting Used to Vanilla Numpy
Gain familiarity with pure numpy.

# Results:

### The Training Accuracy and Loss

![The Training Accuracy and Loss](./docs/training.png?raw=true "The Training Accuracy and Loss")

#### The final test accuracy for a 2 layer FNN is 96.39% for a training of 100 epochs.
